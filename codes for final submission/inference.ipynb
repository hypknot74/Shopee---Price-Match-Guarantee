{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "historic-morrison",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:49:25.739292Z",
     "iopub.status.busy": "2021-05-10T13:49:25.738123Z",
     "iopub.status.idle": "2021-05-10T13:50:30.168525Z",
     "shell.execute_reply": "2021-05-10T13:50:30.168979Z"
    },
    "papermill": {
     "duration": 64.47197,
     "end_time": "2021-05-10T13:50:30.169438",
     "exception": false,
     "start_time": "2021-05-10T13:49:25.697468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/efficientnetkerasapplications/Keras_Applications-1.0.8-py3-none-any.whl\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from Keras-Applications==1.0.8) (2.10.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from Keras-Applications==1.0.8) (1.19.5)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->Keras-Applications==1.0.8) (1.15.0)\r\n",
      "Installing collected packages: Keras-Applications\r\n",
      "Successfully installed Keras-Applications-1.0.8\r\n",
      "Processing /kaggle/input/efficientnetkerasapplications/efficientnet-1.1.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.1) (1.0.8)\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.1) (0.18.1)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.19.5)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (2.10.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.15.0)\r\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (7.2.0)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (2021.4.8)\r\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (2.9.0)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (3.4.1)\r\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (1.5.4)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (1.1.1)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (2.5)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (0.10.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.8.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.4.7)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (1.3.1)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.1.1) (4.4.2)\r\n",
      "Installing collected packages: efficientnet\r\n",
      "Successfully installed efficientnet-1.1.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/efficientnetkerasapplications/Keras_Applications-1.0.8-py3-none-any.whl\n",
    "!pip install ../input/efficientnetkerasapplications/efficientnet-1.1.1-py3-none-any.whl\n",
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml import PCA\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras as efn\n",
    "import math\n",
    "from shutil import copyfile\n",
    "copyfile(src = \"../input/bert-and-tokenization/tokenization.py\", dst = \"../working/tokenization.py\")\n",
    "import tokenization\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import fasttext as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-mineral",
   "metadata": {
    "papermill": {
     "duration": 0.02211,
     "end_time": "2021-05-10T13:50:30.214238",
     "exception": false,
     "start_time": "2021-05-10T13:50:30.192128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CVしない場合True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "strategic-hobby",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:50:30.262513Z",
     "iopub.status.busy": "2021-05-10T13:50:30.262011Z",
     "iopub.status.idle": "2021-05-10T13:50:30.265908Z",
     "shell.execute_reply": "2021-05-10T13:50:30.265467Z"
    },
    "papermill": {
     "duration": 0.029644,
     "end_time": "2021-05-10T13:50:30.266013",
     "exception": false,
     "start_time": "2021-05-10T13:50:30.236369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUICK_SUBMISSION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "north-bowling",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:50:30.316253Z",
     "iopub.status.busy": "2021-05-10T13:50:30.315009Z",
     "iopub.status.idle": "2021-05-10T13:50:30.317288Z",
     "shell.execute_reply": "2021-05-10T13:50:30.317747Z"
    },
    "papermill": {
     "duration": 0.02946,
     "end_time": "2021-05-10T13:50:30.317885",
     "exception": false,
     "start_time": "2021-05-10T13:50:30.288425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = [512, 512]\n",
    "# Seed\n",
    "SEED = 42\n",
    "# Verbosity\n",
    "VERBOSE = 1\n",
    "# Number of classes\n",
    "N_CLASSES = 11014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "iraqi-massachusetts",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:50:38.565258Z",
     "iopub.status.busy": "2021-05-10T13:50:38.564594Z",
     "iopub.status.idle": "2021-05-10T13:50:38.568659Z",
     "shell.execute_reply": "2021-05-10T13:50:38.569041Z"
    },
    "papermill": {
     "duration": 8.228966,
     "end_time": "2021-05-10T13:50:38.569192",
     "exception": false,
     "start_time": "2021-05-10T13:50:30.340226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will restrict TensorFlow to max 2GB GPU RAM\n",
      "then RAPIDS can use 14GB GPU RAM\n"
     ]
    }
   ],
   "source": [
    "# RESTRICT TENSORFLOW TO 2GB OF GPU RAM\n",
    "# SO THAT WE HAVE 14GB RAM FOR RAPIDS\n",
    "LIMIT = 2.0\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "print('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\n",
    "print('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bored-receptor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:50:38.630604Z",
     "iopub.status.busy": "2021-05-10T13:50:38.630085Z",
     "iopub.status.idle": "2021-05-10T13:50:38.658540Z",
     "shell.execute_reply": "2021-05-10T13:50:38.657705Z"
    },
    "papermill": {
     "duration": 0.066486,
     "end_time": "2021-05-10T13:50:38.658665",
     "exception": false,
     "start_time": "2021-05-10T13:50:38.592179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GET_CV = True\n",
    "# Flag to check ram allocations (debug)\n",
    "CHECK_SUB = False\n",
    "\n",
    "df = cudf.read_csv('../input/shopee-product-matching/test.csv')\n",
    "\n",
    "# inferenceをせずにサブミットだけする場合\n",
    "if len(df)==3 and QUICK_SUBMISSION:\n",
    "    #df.to_csv('submission.csv', index=False)\n",
    "    #exit()\n",
    "    GET_CV = False\n",
    "\n",
    "# If we are comitting, replace train set for test set and dont get cv\n",
    "if len(df) > 3:\n",
    "    GET_CV = False\n",
    "del df\n",
    "\n",
    "# Function to get our f1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1\n",
    "\n",
    "\n",
    "# Function to combine predictions\n",
    "def pre_combine_predictions(row):\n",
    "    #x = np.concatenate([row['image_predictions'], row['text_predictions'], row['oof_text'], row['oof_hash'], row['oof_image']])\n",
    "    x = np.concatenate([ row['text_predictions'], row['oof_text'], row['oof_image']])  # hashなし!\n",
    "    return ' '.join( np.unique(x) )\n",
    "\n",
    "\n",
    "# Function to combine predictions\n",
    "def combine_predictions(row):\n",
    "    #x = np.concatenate([row['image_predictions'], row['text_predictions'], row['oof_text'], row['oof_hash'], row['oof_image']])\n",
    "    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['oof_text'], row['oof_image']])  # hashなし!\n",
    "    if len(x)>50:\n",
    "        x = x[:50]\n",
    "    return ' '.join( np.unique(x) )\n",
    "\n",
    "\n",
    "# Function to read out dataset\n",
    "def read_dataset():\n",
    "    if GET_CV:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
    "        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "        df['matches'] = df['label_group'].map(tmp)\n",
    "        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
    "        if CHECK_SUB:\n",
    "            df = pd.concat([df, df], axis = 0)\n",
    "            df.reset_index(drop = True, inplace = True)\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n",
    "    else:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n",
    "        \n",
    "    return df, df_cu, image_paths\n",
    "\n",
    "\n",
    "# Function to decode our images\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "# Function to read our test image and return image\n",
    "def read_image(image):\n",
    "    image = tf.io.read_file(image)\n",
    "    image = decode_image(image)\n",
    "    return image\n",
    "\n",
    "# Function to get our dataset that read images\n",
    "def get_dataset(image):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image)\n",
    "    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "proprietary-villa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:50:38.719939Z",
     "iopub.status.busy": "2021-05-10T13:50:38.718663Z",
     "iopub.status.idle": "2021-05-10T13:50:38.721034Z",
     "shell.execute_reply": "2021-05-10T13:50:38.721512Z"
    },
    "papermill": {
     "duration": 0.039621,
     "end_time": "2021-05-10T13:50:38.721628",
     "exception": false,
     "start_time": "2021-05-10T13:50:38.682007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Arcmarginproduct class keras layer\n",
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "            blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "        \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "julian-medicine",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:50:38.781980Z",
     "iopub.status.busy": "2021-05-10T13:50:38.776644Z",
     "iopub.status.idle": "2021-05-10T13:50:38.784331Z",
     "shell.execute_reply": "2021-05-10T13:50:38.783919Z"
    },
    "papermill": {
     "duration": 0.040056,
     "end_time": "2021-05-10T13:50:38.784438",
     "exception": false,
     "start_time": "2021-05-10T13:50:38.744382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get the embeddings of our images with the fine-tuned model\n",
    "def get_image_embeddings(image_paths):\n",
    "    embeds = []\n",
    "    \n",
    "    margin = ArcMarginProduct(\n",
    "            n_classes = N_CLASSES, \n",
    "            s = 30, \n",
    "            m = 0.5, \n",
    "            name='head/arc_margin', \n",
    "            dtype='float32'\n",
    "            )\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
    "    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
    "    x = efn.EfficientNetB7(weights = None, include_top = False)(inp)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = margin([x, label])\n",
    "        \n",
    "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
    "    model.load_weights('../input/noisystudentefn7-augment/EfficientNetB7_ns_512_42_06-3.70-5.13.h5')\n",
    "    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\n",
    "    chunk = 5000\n",
    "    iterator = np.arange(np.ceil(len(df) / chunk))\n",
    "    for j in iterator:\n",
    "        a = int(j * chunk)\n",
    "        b = int((j + 1) * chunk)\n",
    "        image_dataset = get_dataset(image_paths[a:b])\n",
    "        image_embeddings = model.predict(image_dataset)\n",
    "        embeds.append(image_embeddings)\n",
    "    del model\n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings\n",
    "\n",
    "# Return tokens, masks and segments from a text array or series\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accepting-subdivision",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:50:38.844421Z",
     "iopub.status.busy": "2021-05-10T13:50:38.843152Z",
     "iopub.status.idle": "2021-05-10T13:50:38.845523Z",
     "shell.execute_reply": "2021-05-10T13:50:38.845946Z"
    },
    "papermill": {
     "duration": 0.038114,
     "end_time": "2021-05-10T13:50:38.846061",
     "exception": false,
     "start_time": "2021-05-10T13:50:38.807947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get our text title embeddings using a pre-trained bert model\n",
    "def get_text_embeddings(df, max_len = 70):\n",
    "    embeds = []\n",
    "    module_url = \"../input/bert-en-uncased-l24-h1024-a16-1\"\n",
    "    bert_layer = hub.KerasLayer(module_url, trainable = True)\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "    text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\n",
    "    \n",
    "    margin = ArcMarginProduct(\n",
    "            n_classes = 11014, \n",
    "            s = 30, \n",
    "            m = 0.5, \n",
    "            name='head/arc_margin', \n",
    "            dtype='float32'\n",
    "            )\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    label = tf.keras.layers.Input(shape = (), name = 'label')\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    x = margin([clf_output, label])\n",
    "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n",
    "    \n",
    "    model.load_weights('../input/bert-and-tokenization/Bert_123.h5')\n",
    "    model = tf.keras.models.Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\n",
    "    chunk = 5000\n",
    "    iterator = np.arange(np.ceil(len(df) / chunk))\n",
    "    for j in iterator:\n",
    "        a = int(j * chunk)\n",
    "        b = int((j + 1) * chunk)\n",
    "        text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\n",
    "        text_embeddings = model.predict(text_chunk, batch_size = BATCH_SIZE)\n",
    "        embeds.append(text_embeddings)\n",
    "    del model\n",
    "    text_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our text embeddings shape is {text_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "angry-lunch",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:50:38.909251Z",
     "iopub.status.busy": "2021-05-10T13:50:38.894723Z",
     "iopub.status.idle": "2021-05-10T13:50:38.911671Z",
     "shell.execute_reply": "2021-05-10T13:50:38.911273Z"
    },
    "papermill": {
     "duration": 0.042602,
     "end_time": "2021-05-10T13:50:38.911791",
     "exception": false,
     "start_time": "2021-05-10T13:50:38.869189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get 50 nearest neighbors of each image and apply a distance threshold to maximize cv\n",
    "def get_neighbors(df, embeddings, KNN = 50, image = True):\n",
    "    model = NearestNeighbors(n_neighbors = KNN)\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "\n",
    "    predictions = []\n",
    "    if len(df) == 3 and QUICK_SUBMISSION:\n",
    "        for k in tqdm(range(embeddings.shape[0])):\n",
    "            if image:      \n",
    "                image_threshold = 3.3\n",
    "                ids = np.array([])\n",
    "                idx = np.where(distances[k,] < image_threshold)[0]\n",
    "                ids = indices[k,idx]\n",
    "                posting_ids = df['posting_id'].iloc[ids].values\n",
    "                predictions.append(posting_ids)        \n",
    "            else:\n",
    "                ids = np.array([])\n",
    "                idx = np.where(distances[k,] < 16.0)[0]\n",
    "                ids = indices[k,idx]\n",
    "                if (len(idx)>1):\n",
    "                    arr = distances[k,np.where(distances[k,]<16.0)[0]][1:]\n",
    "                    mean = np.mean(arr)\n",
    "                    standard_deviation = np.std(arr)\n",
    "                    if(standard_deviation>0):\n",
    "                        distance_from_mean = abs(arr - mean)\n",
    "                        max_deviations = 2\n",
    "                        not_outlier = distance_from_mean < max_deviations * standard_deviation\n",
    "                        max_dist = arr[not_outlier][-1]\n",
    "                        idx = np.where(distances[k,] <= max_dist)[0]\n",
    "                        ids = indices[k,idx]\n",
    "                posting_ids = df['posting_id'].iloc[ids].values\n",
    "                predictions.append(posting_ids)\n",
    "    else:\n",
    "        for k in tqdm(range(embeddings.shape[0])):\n",
    "            if image:      \n",
    "                image_threshold = 3.3\n",
    "                ids = np.array([])\n",
    "                idx = np.where(distances[k,] < image_threshold)[0]\n",
    "                if len(idx)==1 and k in not_matched_list:\n",
    "                    while len(idx)<2:\n",
    "                        image_threshold += 0.001\n",
    "                        idx = np.where(distances[k,] < image_threshold)[0]\n",
    "                        if len(idx)>2:\n",
    "                            idx = idx[:2]\n",
    "                ids = indices[k,idx]\n",
    "                posting_ids = df['posting_id'].iloc[ids].values\n",
    "                predictions.append(posting_ids)        \n",
    "            else:\n",
    "                ids = np.array([])\n",
    "                idx = np.where(distances[k,] < 16.0)[0]\n",
    "                ids = indices[k,idx]\n",
    "                if (len(idx)>1):\n",
    "                    arr = distances[k,np.where(distances[k,]<16.0)[0]][1:]\n",
    "                    mean = np.mean(arr)\n",
    "                    standard_deviation = np.std(arr)\n",
    "                    if(standard_deviation>0):\n",
    "                        distance_from_mean = abs(arr - mean)\n",
    "                        max_deviations = 2\n",
    "                        not_outlier = distance_from_mean < max_deviations * standard_deviation\n",
    "                        max_dist = arr[not_outlier][-1]\n",
    "                        idx = np.where(distances[k,] <= max_dist)[0]\n",
    "                        ids = indices[k,idx]\n",
    "                posting_ids = df['posting_id'].iloc[ids].values\n",
    "                predictions.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return df, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-invention",
   "metadata": {
    "papermill": {
     "duration": 0.023155,
     "end_time": "2021-05-10T13:50:38.958351",
     "exception": false,
     "start_time": "2021-05-10T13:50:38.935196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "promising-municipality",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:50:39.018671Z",
     "iopub.status.busy": "2021-05-10T13:50:39.018167Z",
     "iopub.status.idle": "2021-05-10T13:51:29.111938Z",
     "shell.execute_reply": "2021-05-10T13:51:29.112367Z"
    },
    "papermill": {
     "duration": 50.130947,
     "end_time": "2021-05-10T13:51:29.112567",
     "exception": false,
     "start_time": "2021-05-10T13:50:38.981620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our text embeddings shape is (3, 1024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "370597"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, df_cu, image_paths = read_dataset()\n",
    "text_embeddings = get_text_embeddings(df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "center-moisture",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:29.166506Z",
     "iopub.status.busy": "2021-05-10T13:51:29.164993Z",
     "iopub.status.idle": "2021-05-10T13:51:30.107643Z",
     "shell.execute_reply": "2021-05-10T13:51:30.107017Z"
    },
    "papermill": {
     "duration": 0.970354,
     "end_time": "2021-05-10T13:51:30.107828",
     "exception": false,
     "start_time": "2021-05-10T13:51:29.137474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3880.02it/s]\n"
     ]
    }
   ],
   "source": [
    "KNN = 3 if len(df)==3 and QUICK_SUBMISSION else 25\n",
    "df, text_predictions = get_neighbors(df, text_embeddings, KNN = KNN, image = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "special-punishment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:30.163721Z",
     "iopub.status.busy": "2021-05-10T13:51:30.163185Z",
     "iopub.status.idle": "2021-05-10T13:51:44.894121Z",
     "shell.execute_reply": "2021-05-10T13:51:44.893315Z"
    },
    "papermill": {
     "duration": 14.760308,
     "end_time": "2021-05-10T13:51:44.894261",
     "exception": false,
     "start_time": "2021-05-10T13:51:30.133953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text embeddings shape (3, 28)\n"
     ]
    }
   ],
   "source": [
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "model = TfidfVectorizer(stop_words=None, binary=True, max_features=25000)\n",
    "text_embeddings2 = model.fit_transform(df_cu.title).toarray()\n",
    "print('text embeddings shape',text_embeddings2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "enormous-police",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:44.952669Z",
     "iopub.status.busy": "2021-05-10T13:51:44.952152Z",
     "iopub.status.idle": "2021-05-10T13:51:45.217278Z",
     "shell.execute_reply": "2021-05-10T13:51:45.216838Z"
    },
    "papermill": {
     "duration": 0.297692,
     "end_time": "2021-05-10T13:51:45.217400",
     "exception": false,
     "start_time": "2021-05-10T13:51:44.919708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar titles...\n",
      "chunk 0 to 3\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "CHUNK = 1024*4\n",
    "\n",
    "print('Finding similar titles...')\n",
    "CTS = len(df_cu)//CHUNK\n",
    "if len(df_cu)%CHUNK!=0: CTS += 1\n",
    "for j in range( CTS ):\n",
    "    \n",
    "    a = j*CHUNK\n",
    "    b = (j+1)*CHUNK\n",
    "    b = min(b,len(df_cu))\n",
    "    print('chunk',a,'to',b)\n",
    "    \n",
    "    # COSINE SIMILARITY DISTANCE\n",
    "    # cts = np.dot( text_embeddings, text_embeddings[a:b].T).T\n",
    "    cts = cupy.matmul(text_embeddings2, text_embeddings2[a:b].T).T\n",
    "    \n",
    "    for k in range(b-a):\n",
    "        # IDX = np.where(cts[k,]>0.7)[0]\n",
    "        IDX = cupy.where(cts[k,]>0.75)[0]\n",
    "        o = df_cu.iloc[cupy.asnumpy(IDX)].posting_id.to_pandas().values\n",
    "        preds.append(o)\n",
    "        \n",
    "del model, text_embeddings2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "split-sodium",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:45.280661Z",
     "iopub.status.busy": "2021-05-10T13:51:45.280028Z",
     "iopub.status.idle": "2021-05-10T13:51:45.283090Z",
     "shell.execute_reply": "2021-05-10T13:51:45.282670Z"
    },
    "papermill": {
     "duration": 0.036232,
     "end_time": "2021-05-10T13:51:45.283196",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.246964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cu['oof_text'] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-ethiopia",
   "metadata": {
    "papermill": {
     "duration": 0.029195,
     "end_time": "2021-05-10T13:51:45.340874",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.311679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "visible-sixth",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:45.403115Z",
     "iopub.status.busy": "2021-05-10T13:51:45.401722Z",
     "iopub.status.idle": "2021-05-10T13:51:45.404743Z",
     "shell.execute_reply": "2021-05-10T13:51:45.404319Z"
    },
    "papermill": {
     "duration": 0.035416,
     "end_time": "2021-05-10T13:51:45.404897",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.369481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A \n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "strong-hours",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:45.464450Z",
     "iopub.status.busy": "2021-05-10T13:51:45.463254Z",
     "iopub.status.idle": "2021-05-10T13:51:45.466051Z",
     "shell.execute_reply": "2021-05-10T13:51:45.465600Z"
    },
    "papermill": {
     "duration": 0.033096,
     "end_time": "2021-05-10T13:51:45.466157",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.433061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    img_size = 512\n",
    "    batch_size = 12\n",
    "    seed = 2020\n",
    "    \n",
    "    device = 'cuda'\n",
    "    classes = 11014\n",
    "    \n",
    "    model_name = 'eca_nfnet_l0'\n",
    "    model_path = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n",
    "    \n",
    "    scale = 30 \n",
    "    margin = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "facial-feedback",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:45.527015Z",
     "iopub.status.busy": "2021-05-10T13:51:45.526350Z",
     "iopub.status.idle": "2021-05-10T13:51:45.533162Z",
     "shell.execute_reply": "2021-05-10T13:51:45.532723Z"
    },
    "papermill": {
     "duration": 0.039512,
     "end_time": "2021-05-10T13:51:45.533258",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.493746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lonely-quest",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:45.597887Z",
     "iopub.status.busy": "2021-05-10T13:51:45.597151Z",
     "iopub.status.idle": "2021-05-10T13:51:45.600067Z",
     "shell.execute_reply": "2021-05-10T13:51:45.599540Z"
    },
    "papermill": {
     "duration": 0.037864,
     "end_time": "2021-05-10T13:51:45.600186",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.562322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_predictions(df, embeddings,threshold = 0.0):\n",
    "    \n",
    "    if len(df) > 3:\n",
    "        KNN = 50\n",
    "    else : \n",
    "        KNN = 3\n",
    "    \n",
    "    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "    predictions = []\n",
    "    for k in tqdm(range(embeddings.shape[0])):\n",
    "        idx = np.where(distances[k,] < threshold)[0]\n",
    "        ids = indices[k,idx]\n",
    "        posting_ids = df['posting_id'].iloc[ids].values\n",
    "        predictions.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "optional-interstate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:45.658036Z",
     "iopub.status.busy": "2021-05-10T13:51:45.657368Z",
     "iopub.status.idle": "2021-05-10T13:51:45.660197Z",
     "shell.execute_reply": "2021-05-10T13:51:45.659809Z"
    },
    "papermill": {
     "duration": 0.033044,
     "end_time": "2021-05-10T13:51:45.660298",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.627254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_transforms():\n",
    "\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n",
    "            A.Normalize(),\n",
    "        ToTensorV2(p=1.0)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "intensive-throw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:45.718054Z",
     "iopub.status.busy": "2021-05-10T13:51:45.717405Z",
     "iopub.status.idle": "2021-05-10T13:51:45.720088Z",
     "shell.execute_reply": "2021-05-10T13:51:45.719681Z"
    },
    "papermill": {
     "duration": 0.033965,
     "end_time": "2021-05-10T13:51:45.720199",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.686234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, image_paths, transforms=None):\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.augmentations = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_paths.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']       \n",
    "    \n",
    "        return image,torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "alike-upper",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:45.783551Z",
     "iopub.status.busy": "2021-05-10T13:51:45.782808Z",
     "iopub.status.idle": "2021-05-10T13:51:45.785405Z",
     "shell.execute_reply": "2021-05-10T13:51:45.785006Z"
    },
    "papermill": {
     "duration": 0.039149,
     "end_time": "2021-05-10T13:51:45.785506",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.746357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct_Image(nn.Module):\n",
    "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct_Image, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "        \n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.scale\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "precious-programmer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:45.851717Z",
     "iopub.status.busy": "2021-05-10T13:51:45.851025Z",
     "iopub.status.idle": "2021-05-10T13:51:45.853830Z",
     "shell.execute_reply": "2021-05-10T13:51:45.853327Z"
    },
    "papermill": {
     "duration": 0.042166,
     "end_time": "2021-05-10T13:51:45.853929",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.811763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes = CFG.classes,\n",
    "        model_name = CFG.model_name,\n",
    "        fc_dim = 512,\n",
    "        margin = CFG.margin,\n",
    "        scale = CFG.scale,\n",
    "        use_fc = True,\n",
    "        pretrained = False):\n",
    "\n",
    "\n",
    "        super(ShopeeModel,self).__init__()\n",
    "        print('Building Model Backbone for {} model'.format(model_name))\n",
    "        \n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "        if model_name == 'resnext50_32x4d':\n",
    "            final_in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "\n",
    "        elif model_name == 'efficientnet_b3':\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "\n",
    "        elif model_name == 'tf_efficientnet_b5_ns':\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            \n",
    "        elif model_name == 'eca_nfnet_l0':\n",
    "            final_in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "            self.backbone.head.global_pool = nn.Identity()\n",
    "\n",
    "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.use_fc = use_fc\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "        self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self._init_params()\n",
    "        final_in_features = fc_dim\n",
    "\n",
    "        self.final = ArcMarginProduct_Image(\n",
    "            final_in_features,\n",
    "            n_classes,\n",
    "            scale = scale,\n",
    "            margin = margin,\n",
    "            easy_margin = False,\n",
    "            ls_eps = 0.0\n",
    "        )\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "        \n",
    "    def forward(self, image, label):\n",
    "        feature = self.extract_feat(image)\n",
    "        #logits = self.final(feature,label)\n",
    "        return feature\n",
    "\n",
    "    def extract_feat(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "\n",
    "        if self.use_fc:\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "prepared-china",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:45.916238Z",
     "iopub.status.busy": "2021-05-10T13:51:45.915572Z",
     "iopub.status.idle": "2021-05-10T13:51:45.919045Z",
     "shell.execute_reply": "2021-05-10T13:51:45.918615Z"
    },
    "papermill": {
     "duration": 0.038661,
     "end_time": "2021-05-10T13:51:45.919152",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.880491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mish_func(torch.autograd.Function):\n",
    "    \n",
    "    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.tanh(F.softplus(i))\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "  \n",
    "        v = 1. + i.exp()\n",
    "        h = v.log() \n",
    "        grad_gh = 1./h.cosh().pow_(2)\n",
    "        \n",
    "        # Note that grad_hv * grad_vx = sigmoid(x)\n",
    "        #grad_hv = 1./v  \n",
    "        #grad_vx = i.exp()\n",
    "        \n",
    "        grad_hx = i.sigmoid()\n",
    "\n",
    "        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n",
    "        \n",
    "        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n",
    "        \n",
    "        return grad_output * grad_f \n",
    "    \n",
    "class Mish(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def forward(self, input_tensor):\n",
    "        return Mish_func.apply(input_tensor)\n",
    "    \n",
    "    \n",
    "def replace_activations(model, existing_layer, new_layer):\n",
    "    \n",
    "    \"\"\"A function for replacing existing activation layers\"\"\"\n",
    "    \n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        if len(list(module.children())) > 0:\n",
    "            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n",
    "\n",
    "        if type(module) == existing_layer:\n",
    "            layer_old = module\n",
    "            layer_new = new_layer\n",
    "            model._modules[name] = layer_new\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stainless-basketball",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:45.981198Z",
     "iopub.status.busy": "2021-05-10T13:51:45.980521Z",
     "iopub.status.idle": "2021-05-10T13:51:45.983633Z",
     "shell.execute_reply": "2021-05-10T13:51:45.984177Z"
    },
    "papermill": {
     "duration": 0.037454,
     "end_time": "2021-05-10T13:51:45.984317",
     "exception": false,
     "start_time": "2021-05-10T13:51:45.946863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_embeddings1(image_paths, model_name = CFG.model_name):\n",
    "    embeds = []\n",
    "    \n",
    "    model = ShopeeModel(model_name = model_name)\n",
    "    model.eval()\n",
    "    \n",
    "    if model_name == 'eca_nfnet_l0':\n",
    "        model = replace_activations(model, torch.nn.SiLU, Mish())\n",
    "\n",
    "    model.load_state_dict(torch.load(CFG.model_path))\n",
    "    model = model.to(CFG.device)\n",
    "    \n",
    "\n",
    "    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n",
    "    image_loader = torch.utils.data.DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img,label in tqdm(image_loader): \n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "            feat = model(img,label)\n",
    "            image_embeddings = feat.detach().cpu().numpy()\n",
    "            embeds.append(image_embeddings)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    \n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "incorrect-election",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:46.050862Z",
     "iopub.status.busy": "2021-05-10T13:51:46.050330Z",
     "iopub.status.idle": "2021-05-10T13:51:46.068080Z",
     "shell.execute_reply": "2021-05-10T13:51:46.067310Z"
    },
    "papermill": {
     "duration": 0.054014,
     "end_time": "2021-05-10T13:51:46.068205",
     "exception": false,
     "start_time": "2021-05-10T13:51:46.014191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>0006c8e5462ae52167402bac1c2e916e.jpg</td>\n",
       "      <td>ecc292392dc7687a</td>\n",
       "      <td>Edufuntoys - CHARACTER PHONE ada lampu dan mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>0007585c4d0f932859339129f709bfdc.jpg</td>\n",
       "      <td>e9968f60d2699e2c</td>\n",
       "      <td>(Beli 1 Free Spatula) Masker Komedo | Blackhea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>0008377d3662e83ef44e1881af38b879.jpg</td>\n",
       "      <td>ba81c17e3581cabe</td>\n",
       "      <td>READY Lemonilo Mie instant sehat kuah dan goreng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id                                 image       image_phash  \\\n",
       "0  test_2255846744  0006c8e5462ae52167402bac1c2e916e.jpg  ecc292392dc7687a   \n",
       "1  test_3588702337  0007585c4d0f932859339129f709bfdc.jpg  e9968f60d2699e2c   \n",
       "2  test_4015706929  0008377d3662e83ef44e1881af38b879.jpg  ba81c17e3581cabe   \n",
       "\n",
       "                                               title  \n",
       "0  Edufuntoys - CHARACTER PHONE ada lampu dan mus...  \n",
       "1  (Beli 1 Free Spatula) Masker Komedo | Blackhea...  \n",
       "2   READY Lemonilo Mie instant sehat kuah dan goreng  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_image,df_image_cu,image_paths = read_dataset()\n",
    "df_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "streaming-earthquake",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:46.129241Z",
     "iopub.status.busy": "2021-05-10T13:51:46.128629Z",
     "iopub.status.idle": "2021-05-10T13:51:51.013833Z",
     "shell.execute_reply": "2021-05-10T13:51:51.013048Z"
    },
    "papermill": {
     "duration": 4.917374,
     "end_time": "2021-05-10T13:51:51.014014",
     "exception": false,
     "start_time": "2021-05-10T13:51:46.096640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model Backbone for eca_nfnet_l0 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3404.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is (3, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_embeddings1 = get_image_embeddings1(image_paths.values)\n",
    "image_predictions1 = get_image_predictions(df_image, image_embeddings1, threshold = 0.26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-airplane",
   "metadata": {
    "papermill": {
     "duration": 0.03325,
     "end_time": "2021-05-10T13:51:51.084283",
     "exception": false,
     "start_time": "2021-05-10T13:51:51.051033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**一旦採点**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "broken-penny",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:51.155274Z",
     "iopub.status.busy": "2021-05-10T13:51:51.154419Z",
     "iopub.status.idle": "2021-05-10T13:51:51.167756Z",
     "shell.execute_reply": "2021-05-10T13:51:51.167343Z"
    },
    "papermill": {
     "duration": 0.051406,
     "end_time": "2021-05-10T13:51:51.167867",
     "exception": false,
     "start_time": "2021-05-10T13:51:51.116461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Concatenate image predctions with text predictions\n",
    "df['oof_text'] = df_cu['oof_text'].to_pandas().values\n",
    "df['text_predictions'] = text_predictions\n",
    "df['oof_image']=image_predictions1\n",
    "df['pre_matches'] = df.apply(pre_combine_predictions, axis = 1)\n",
    "df['matches_num']=df['pre_matches'].map(lambda x: len(x.split(' ')))\n",
    "not_matched_list = df.index[df['matches_num']==1].tolist()\n",
    "print(len(not_matched_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-guinea",
   "metadata": {
    "papermill": {
     "duration": 0.030232,
     "end_time": "2021-05-10T13:51:51.228824",
     "exception": false,
     "start_time": "2021-05-10T13:51:51.198592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TF image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "center-chain",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:51:51.293347Z",
     "iopub.status.busy": "2021-05-10T13:51:51.292860Z",
     "iopub.status.idle": "2021-05-10T13:52:10.994257Z",
     "shell.execute_reply": "2021-05-10T13:52:10.993720Z"
    },
    "papermill": {
     "duration": 19.735147,
     "end_time": "2021-05-10T13:52:10.994389",
     "exception": false,
     "start_time": "2021-05-10T13:51:51.259242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is (3, 2560)\n"
     ]
    }
   ],
   "source": [
    "image_embeddings = get_image_embeddings(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "isolated-garbage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:52:11.062626Z",
     "iopub.status.busy": "2021-05-10T13:52:11.061709Z",
     "iopub.status.idle": "2021-05-10T13:52:11.515657Z",
     "shell.execute_reply": "2021-05-10T13:52:11.515149Z"
    },
    "papermill": {
     "duration": 0.489983,
     "end_time": "2021-05-10T13:52:11.515817",
     "exception": false,
     "start_time": "2021-05-10T13:52:11.025834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 3130.08it/s]\n"
     ]
    }
   ],
   "source": [
    "KNN = 3 if len(df)==3 and QUICK_SUBMISSION else 25\n",
    "df, image_predictions = get_neighbors(df, image_embeddings, KNN = KNN, image = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "distant-coalition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T13:52:11.590135Z",
     "iopub.status.busy": "2021-05-10T13:52:11.589306Z",
     "iopub.status.idle": "2021-05-10T13:52:11.602366Z",
     "shell.execute_reply": "2021-05-10T13:52:11.601963Z"
    },
    "papermill": {
     "duration": 0.054259,
     "end_time": "2021-05-10T13:52:11.602477",
     "exception": false,
     "start_time": "2021-05-10T13:52:11.548218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate image predctions with text predictions\n",
    "if GET_CV:\n",
    "    df['image_predictions'] = image_predictions\n",
    "    df['text_predictions'] = text_predictions\n",
    "    df['oof_text'] = df_cu['oof_text'].to_pandas().values\n",
    "    df['pred_matches'] = df.apply(combine_predictions, axis = 1)\n",
    "    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "    score = df['f1'].mean()\n",
    "    print(f'Our final f1 cv score is {score}')\n",
    "    df['matches'] = df['pred_matches']\n",
    "    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\n",
    "else:\n",
    "    df['image_predictions'] = image_predictions\n",
    "    df['oof_text'] = df_cu['oof_text'].to_pandas().values\n",
    "    df['text_predictions'] = text_predictions\n",
    "    df['oof_image']=image_predictions1\n",
    "    df['matches'] = df.apply(combine_predictions, axis = 1)\n",
    "    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-chart",
   "metadata": {
    "papermill": {
     "duration": 0.031444,
     "end_time": "2021-05-10T13:52:11.665450",
     "exception": false,
     "start_time": "2021-05-10T13:52:11.634006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 176.024842,
   "end_time": "2021-05-10T13:52:15.328649",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-10T13:49:19.303807",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
