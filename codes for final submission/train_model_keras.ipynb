{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "promotional-institution",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-07T09:13:09.548287Z",
     "iopub.status.busy": "2021-05-07T09:13:09.535841Z",
     "iopub.status.idle": "2021-05-07T09:13:32.136619Z",
     "shell.execute_reply": "2021-05-07T09:13:32.137193Z"
    },
    "papermill": {
     "duration": 22.624199,
     "end_time": "2021-05-07T09:13:32.137532",
     "exception": false,
     "start_time": "2021-05-07T09:13:09.513333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in /opt/conda/lib/python3.7/site-packages (0.12.1)\r\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow_addons) (2.12.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q efficientnet\n",
    "!pip install tensorflow_addons\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm.notebook import tqdm\n",
    "from kaggle_datasets import KaggleDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exceptional-pride",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-07T09:13:32.153981Z",
     "iopub.status.busy": "2021-05-07T09:13:32.153360Z",
     "iopub.status.idle": "2021-05-07T09:13:38.165477Z",
     "shell.execute_reply": "2021-05-07T09:13:38.164831Z"
    },
    "papermill": {
     "duration": 6.021069,
     "end_time": "2021-05-07T09:13:38.165621",
     "exception": false,
     "start_time": "2021-05-07T09:13:32.144552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entertaining-manufacturer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-07T09:13:38.194378Z",
     "iopub.status.busy": "2021-05-07T09:13:38.193676Z",
     "iopub.status.idle": "2021-05-07T09:13:38.642402Z",
     "shell.execute_reply": "2021-05-07T09:13:38.641861Z"
    },
    "papermill": {
     "duration": 0.470138,
     "end_time": "2021-05-07T09:13:38.642555",
     "exception": false,
     "start_time": "2021-05-07T09:13:38.172417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "GCS_PATH = KaggleDatasets().get_gcs_path('shopee-augmented-tf-records-512')\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "IMAGE_SIZE = [384, 384]\n",
    "# Seed\n",
    "SEED = 42\n",
    "# Learning rate\n",
    "LR = 0.0001\n",
    "# Verbosity\n",
    "VERBOSE = 2\n",
    "# Number of classes\n",
    "N_CLASSES = 11014\n",
    "# Number of folds\n",
    "FOLDS = 5\n",
    "\n",
    "# Training filenames directory\n",
    "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/*.tfrec')\n",
    "\n",
    "# Noisy Student EfficientNet\n",
    "BASE_NET = '../input/efficientnet-keras-noisystudent-weights-b0b7/efficientnet-b7_noisy-student_notop.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "annoying-terminal",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-07T09:13:38.670092Z",
     "iopub.status.busy": "2021-05-07T09:13:38.664971Z",
     "iopub.status.idle": "2021-05-07T09:13:38.686125Z",
     "shell.execute_reply": "2021-05-07T09:13:38.685487Z"
    },
    "papermill": {
     "duration": 0.036959,
     "end_time": "2021-05-07T09:13:38.686278",
     "exception": false,
     "start_time": "2021-05-07T09:13:38.649319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 68500 training images\n"
     ]
    }
   ],
   "source": [
    "# Function to get our f1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1\n",
    "\n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "def arcface_format(posting_id, image, label_group, matches):\n",
    "    return posting_id, {'inp1': image, 'inp2': label_group}, label_group, matches\n",
    "\n",
    "# Data augmentation function\n",
    "def data_augment(posting_id, image, label_group, matches):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_hue(image, 0.01)\n",
    "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
    "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
    "    image = tf.image.random_brightness(image, 0.10)\n",
    "    return posting_id, image, label_group, matches\n",
    "\n",
    "# Function to decode our images\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "# This function parse our images and also get the target variable\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"posting_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label_group\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"matches\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    posting_id = example['posting_id']\n",
    "    image = decode_image(example['image'])\n",
    "#     label_group = tf.one_hot(tf.cast(example['label_group'], tf.int32), depth = N_CLASSES)\n",
    "    label_group = tf.cast(example['label_group'], tf.int32)\n",
    "    matches = example['matches']\n",
    "    return posting_id, image, label_group, matches\n",
    "\n",
    "# This function loads TF Records and parse them into tensors\n",
    "def load_dataset(filenames, ordered = False):\n",
    "    \n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False \n",
    "        \n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls = AUTO) \n",
    "    return dataset\n",
    "\n",
    "# This function is to get our training tensors\n",
    "def get_training_dataset(filenames, ordered = False):\n",
    "    dataset = load_dataset(filenames, ordered = ordered)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "# This function is to get our validation tensors\n",
    "def get_validation_dataset(filenames, ordered = True):\n",
    "    dataset = load_dataset(filenames, ordered = ordered)\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) \n",
    "    return dataset\n",
    "\n",
    "# Function to count how many photos we have in\n",
    "def count_data_items(filenames):\n",
    "    # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "print(f'Dataset: {NUM_TRAINING_IMAGES} training images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "matched-hughes",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-07T09:13:38.736125Z",
     "iopub.status.busy": "2021-05-07T09:13:38.734371Z",
     "iopub.status.idle": "2021-05-07T09:55:34.497689Z",
     "shell.execute_reply": "2021-05-07T09:55:34.496941Z"
    },
    "papermill": {
     "duration": 2515.804476,
     "end_time": "2021-05-07T09:55:34.497995",
     "exception": false,
     "start_time": "2021-05-07T09:13:38.693519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "196/196 - 456s - loss: 23.8533 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.8226 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: saving model to EfficientNetB7_ns_384_42_01.h5\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00025680000000000006.\n",
      "196/196 - 216s - loss: 22.2396 - sparse_categorical_accuracy: 1.9930e-05 - val_loss: 19.3898 - val_sparse_categorical_accuracy: 0.0014\n",
      "\n",
      "Epoch 00002: saving model to EfficientNetB7_ns_384_42_02.h5\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0005126000000000001.\n",
      "196/196 - 216s - loss: 15.5044 - sparse_categorical_accuracy: 0.0376 - val_loss: 11.6309 - val_sparse_categorical_accuracy: 0.1278\n",
      "\n",
      "Epoch 00003: saving model to EfficientNetB7_ns_384_42_03.h5\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0007684000000000001.\n",
      "196/196 - 216s - loss: 8.7478 - sparse_categorical_accuracy: 0.1976 - val_loss: 7.7219 - val_sparse_categorical_accuracy: 0.2978\n",
      "\n",
      "Epoch 00004: saving model to EfficientNetB7_ns_384_42_04.h5\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010242.\n",
      "196/196 - 216s - loss: 5.3649 - sparse_categorical_accuracy: 0.3978 - val_loss: 6.1169 - val_sparse_categorical_accuracy: 0.4183\n",
      "\n",
      "Epoch 00005: saving model to EfficientNetB7_ns_384_42_05.h5\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00128.\n",
      "196/196 - 218s - loss: 3.9918 - sparse_categorical_accuracy: 0.5233 - val_loss: 5.5470 - val_sparse_categorical_accuracy: 0.4673\n",
      "\n",
      "Epoch 00006: saving model to EfficientNetB7_ns_384_42_06.h5\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010242.\n",
      "196/196 - 217s - loss: 2.5422 - sparse_categorical_accuracy: 0.6845 - val_loss: 4.4338 - val_sparse_categorical_accuracy: 0.5772\n",
      "\n",
      "Epoch 00007: saving model to EfficientNetB7_ns_384_42_07.h5\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0008195600000000003.\n",
      "196/196 - 216s - loss: 1.6537 - sparse_categorical_accuracy: 0.7978 - val_loss: 3.7401 - val_sparse_categorical_accuracy: 0.6513\n",
      "\n",
      "Epoch 00008: saving model to EfficientNetB7_ns_384_42_08.h5\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0006558480000000003.\n",
      "196/196 - 216s - loss: 1.1423 - sparse_categorical_accuracy: 0.8663 - val_loss: 3.3461 - val_sparse_categorical_accuracy: 0.6988\n",
      "\n",
      "Epoch 00009: saving model to EfficientNetB7_ns_384_42_09.h5\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0005248784000000002.\n",
      "196/196 - 216s - loss: 0.8366 - sparse_categorical_accuracy: 0.9070 - val_loss: 3.0661 - val_sparse_categorical_accuracy: 0.7350\n",
      "\n",
      "Epoch 00010: saving model to EfficientNetB7_ns_384_42_10.h5\n"
     ]
    }
   ],
   "source": [
    "# Function for a custom learning rate scheduler with warmup and decay\n",
    "def get_lr_callback():\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.000005 * BATCH_SIZE\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback\n",
    "\n",
    "# Arcmarginproduct class keras layer\n",
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "            blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output\n",
    "\n",
    "\n",
    "# Function to create our EfficientNetB3 model\n",
    "def get_model():\n",
    "\n",
    "    with strategy.scope():\n",
    "\n",
    "        margin = ArcMarginProduct(\n",
    "            n_classes = N_CLASSES, \n",
    "            s = 30, \n",
    "            m = 0.5, \n",
    "            name='head/arc_margin', \n",
    "            dtype='float32'\n",
    "            )\n",
    "\n",
    "        inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
    "        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
    "        x = efn.EfficientNetB7(weights = BASE_NET, include_top = False)(inp)\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = margin([x, label])\n",
    "        \n",
    "        output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer = opt,\n",
    "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "            ) \n",
    "        \n",
    "        return model\n",
    "\n",
    "def train_and_evaluate():\n",
    "\n",
    "    # Seed everything\n",
    "    seed_everything(SEED)\n",
    "    \n",
    "    print('\\n')\n",
    "    print('-'*50)\n",
    "    train, valid = train_test_split(TRAINING_FILENAMES, shuffle = True, random_state = SEED)\n",
    "    train_dataset = get_training_dataset(train, ordered = False)\n",
    "    train_dataset = train_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
    "    val_dataset = get_validation_dataset(valid, ordered = True)\n",
    "    val_dataset = val_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
    "    STEPS_PER_EPOCH = count_data_items(train) // BATCH_SIZE\n",
    "    K.clear_session()\n",
    "    model = get_model()\n",
    "    # Model checkpoint\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'EfficientNetB7_ns_{IMAGE_SIZE[0]}_{SEED}_'+'{epoch:02d}'+'.h5', \n",
    "                                                    monitor = 'val_loss', \n",
    "                                                    verbose = VERBOSE, \n",
    "                                                    save_best_only = False,\n",
    "                                                    save_weights_only = True, \n",
    "                                                    mode = 'min',\n",
    "                                                    period=1)\n",
    "\n",
    "    history = model.fit(train_dataset,\n",
    "                        steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                        epochs = EPOCHS,\n",
    "                        callbacks = [checkpoint, get_lr_callback()], \n",
    "                        validation_data = val_dataset,\n",
    "                        verbose = VERBOSE)\n",
    "\n",
    "    \n",
    "train_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2555.877471,
   "end_time": "2021-05-07T09:55:37.127797",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-07T09:13:01.250326",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
